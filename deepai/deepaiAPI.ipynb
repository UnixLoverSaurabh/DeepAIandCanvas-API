{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognize famous people within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': {'celebrities': [{'bounding_box': [93, 26, 44, 44], 'confidence': '0.80', 'name': 'katrina kaif'}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/celebrity-recognition\",\n",
    "    files={\n",
    "        'image': open('kat.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and label the age, gender, and cultural appearance of people in an image or video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': {'faces': [{'age_range': [32, 47], 'cultural_appearance_confidence': 0.95, 'gender': 'Female', 'age_range_confidence': 0.88, 'bounding_box': [93, 26, 44, 44], 'gender_confidence': 0.99, 'cultural_appearance': 'Latino'}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/demographic-recognition\",\n",
    "    files={\n",
    "        'image': open('kat.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseCap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions an image by labeling every object the model detects is present within the image. \n",
    "#### The output is the labeled image along with a JSON snippet that includes each label and its coordinates within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': {'captions': [{'caption': 'a woman sitting on a bench', 'bounding_box': [458.35614013672, 51.972095489502, 158.81829833984, 251.86981201172], 'confidence': 3.8059983253479}, {'caption': 'a man wearing a red and white striped shirt', 'bounding_box': [4.6645855903625, 1.5635619163513, 208.93174743652, 316.09893798828], 'confidence': 3.2257552146912}, {'caption': 'man in a white shirt', 'bounding_box': [580.07012939453, 37.135627746582, 184.3098449707, 256.61904907227], 'confidence': 2.9259624481201}, {'caption': 'a man wearing a white shirt', 'bounding_box': [331.95220947266, 38.084819793701, 140.75592041016, 226.38475036621], 'confidence': 1.8413834571838}, {'caption': 'woman wearing a blue shirt', 'bounding_box': [188.84405517578, 32.622611999512, 196.37347412109, 269.60949707031], 'confidence': 1.6036977767944}, {'caption': 'people in a park', 'bounding_box': [240.18707275391, 7.5463061332703, 442.47637939453, 309.5305480957], 'confidence': 1.3990656137466}, {'caption': 'woman with long hair', 'bounding_box': [232.1875, 42.370365142822, 106.52349090576, 130.71296691895], 'confidence': 0.89691507816315}, {'caption': 'a woman with a red hair', 'bounding_box': [513.14294433594, 32.148880004883, 58.413898468018, 113.99152374268], 'confidence': 0.64702177047729}, {'caption': 'black and white striped shirt', 'bounding_box': [21.062629699707, 200.84744262695, 121.73845672607, 78.456932067871], 'confidence': 0.21797266602516}, {'caption': 'a man wearing a red hat', 'bounding_box': [289.36419677734, 18.37557220459, 160.73001098633, 84.007637023926], 'confidence': 0.20052137970924}, {'caption': 'man wearing a white shirt', 'bounding_box': [615.6455078125, 12.334546089172, 178.75720214844, 122.87174987793], 'confidence': 0.060781508684158}, {'caption': 'a red shirt on a man', 'bounding_box': [719.90966796875, 99.203643798828, 78.926414489746, 216.14668273926], 'confidence': -0.079084068536758}, {'caption': 'a woman with a red shirt', 'bounding_box': [431.64270019531, 17.952919006348, 114.9349899292, 155.44288635254], 'confidence': -0.093051612377167}, {'caption': 'a woman wearing a blue shirt', 'bounding_box': [129.9891204834, 50.462718963623, 121.63353729248, 255.88101196289], 'confidence': -0.24240472912788}, {'caption': 'a table with a wooden table', 'bounding_box': [273.55813598633, 204.90351867676, 245.95822143555, 106.76457214355], 'confidence': -0.44319623708725}, {'caption': 'a woman wearing a pink shirt', 'bounding_box': [260.88327026367, 98.640815734863, 152.90185546875, 135.95500183105], 'confidence': -0.47569036483765}, {'caption': 'a man wearing a black and white shirt', 'bounding_box': [2.5423724651337, 14.831254959106, 79.398918151855, 208.82510375977], 'confidence': -0.50297105312347}, {'caption': 'people watching the game', 'bounding_box': [-4.3674869537354, 12.179942131042, 556.80963134766, 156.06864929199], 'confidence': -0.57109743356705}, {'caption': 'a black and white skateboard', 'bounding_box': [414.54425048828, 250.98811340332, 107.79845428467, 61.784683227539], 'confidence': -0.57814586162567}, {'caption': 'a man holding a rope', 'bounding_box': [3.9382004737854, 207.08657836914, 313.55645751953, 106.54218292236], 'confidence': -0.62623995542526}, {'caption': 'a woman wearing a red shirt', 'bounding_box': [550.18200683594, 15.632520675659, 47.695243835449, 96.130790710449], 'confidence': -0.64086306095123}, {'caption': 'a red and white bag', 'bounding_box': [658.86248779297, 197.60383605957, 123.9436340332, 114.52855682373], 'confidence': -0.67345345020294}, {'caption': 'a woman wearing a red shirt', 'bounding_box': [498.17184448242, 14.668690681458, 153.16467285156, 104.53823852539], 'confidence': -0.69998008012772}, {'caption': 'a man wearing a hat', 'bounding_box': [360.70007324219, 8.0278177261353, 115.70796966553, 54.273693084717], 'confidence': -0.79817473888397}, {'caption': 'a small white and black shirt', 'bounding_box': [316.4104309082, 160.78782653809, 109.25522613525, 101.8069229126], 'confidence': -0.80590128898621}, {'caption': 'the hand of a woman', 'bounding_box': [564.92999267578, 192.18843078613, 70.365882873535, 118.04473876953], 'confidence': -0.81514847278595}, {'caption': 'a red and white chair', 'bounding_box': [370.22821044922, 282.46170043945, 432.24310302734, 34.460372924805], 'confidence': -0.85733532905579}, {'caption': 'man wearing a blue hat', 'bounding_box': [399.69900512695, 18.103038787842, 71.018463134766, 127.0213470459], 'confidence': -0.86081254482269}, {'caption': 'a baseball player wearing a helmet', 'bounding_box': [3.3609914779663, 39.906379699707, 40.066631317139, 112.89706420898], 'confidence': -0.89429891109467}, {'caption': 'man wearing a black hat', 'bounding_box': [4.6780905723572, 18.387985229492, 67.271484375, 68.506484985352], 'confidence': -0.93691658973694}, {'caption': 'a white plate on a table', 'bounding_box': [563.85430908203, 296.18743896484, 50.160514831543, 19.38963508606], 'confidence': -0.94310617446899}, {'caption': 'red and white flag', 'bounding_box': [400.48306274414, 4.0306549072266, 96.099670410156, 26.764427185059], 'confidence': -0.99761879444122}, {'caption': 'a man holding a skateboard', 'bounding_box': [195.51707458496, 202.19085693359, 64.611022949219, 110.28758239746], 'confidence': -1.012805223465}, {'caption': 'a trash can', 'bounding_box': [258.73388671875, 221.53732299805, 67.126243591309, 88.850715637207], 'confidence': -1.0247701406479}, {'caption': 'a black and white table', 'bounding_box': [446.78973388672, 279.40161132812, 136.43893432617, 36.466594696045], 'confidence': -1.0264128446579}, {'caption': 'a blue and white plate', 'bounding_box': [390.2678527832, 225.39576721191, 73.617599487305, 82.640068054199], 'confidence': -1.0370651483536}, {'caption': 'a black and white striped zebra', 'bounding_box': [3.1568367481232, 211.52662658691, 49.064643859863, 55.264114379883], 'confidence': -1.054337978363}, {'caption': 'woman wearing a red shirt', 'bounding_box': [466.5426940918, 14.803986549377, 109.64096832275, 57.703929901123], 'confidence': -1.0591974258423}, {'caption': 'a small white plate', 'bounding_box': [494.09036254883, 303.27590942383, 110.52523803711, 13.751065254211], 'confidence': -1.0691293478012}, {'caption': 'a man holding a knife', 'bounding_box': [365.16677856445, 199.07025146484, 435.34384155273, 113.67488861084], 'confidence': -1.0823755264282}, {'caption': 'a person holding a plate', 'bounding_box': [543.07318115234, 273.40090942383, 61.74535369873, 40.728214263916], 'confidence': -1.0877701044083}, {'caption': 'a woman wearing a black skirt', 'bounding_box': [99.291015625, 171.57713317871, 64.171241760254, 138.52479553223], 'confidence': -1.1862063407898}, {'caption': 'a black and white shirt', 'bounding_box': [4.6779761314392, 151.93571472168, 87.488563537598, 109.44460296631], 'confidence': -1.1869864463806}, {'caption': 'a black and white plate', 'bounding_box': [126.68828582764, 299.89840698242, 558.36737060547, 17.396190643311], 'confidence': -1.2061079740524}, {'caption': 'a woman holding a knife', 'bounding_box': [615.18273925781, 197.53117370605, 86.396102905273, 89.231964111328], 'confidence': -1.2875190973282}, {'caption': 'a person holding a tennis racket', 'bounding_box': [570.06604003906, 309.44409179688, 57.974407196045, 7.6101632118225], 'confidence': -1.300931930542}, {'caption': 'a woman holding a tennis racket', 'bounding_box': [569.67572021484, 273.91168212891, 124.32177734375, 40.801628112793], 'confidence': -1.3388366699219}, {'caption': 'a woman wearing a red shirt', 'bounding_box': [507.97717285156, 7.5265851020813, 118.55154418945, 42.14847946167], 'confidence': -1.3434890508652}, {'caption': 'a blue and white plate', 'bounding_box': [331.37948608398, 225.60737609863, 74.047721862793, 84.673942565918], 'confidence': -1.3662283420563}, {'caption': 'a black metal table', 'bounding_box': [90.410118103027, 317.39987182617, 626.38989257812, 0.95987218618393], 'confidence': -1.3886966705322}, {'caption': 'a yellow and black pole', 'bounding_box': [8.5874118804932, 307.38394165039, 326.89559936523, 10.369728088379], 'confidence': -1.3922820091248}, {'caption': 'the arm of a man', 'bounding_box': [4.1395263671875, 162.48237609863, 55.429321289062, 51.250213623047], 'confidence': -1.3948278427124}, {'caption': 'a person holding a bag', 'bounding_box': [224.36256408691, 168.83149719238, 72.794929504395, 132.19104003906], 'confidence': -1.398958325386}, {'caption': 'a woman holding a red umbrella', 'bounding_box': [551.12176513672, 117.9066696167, 91.421684265137, 113.56033325195], 'confidence': -1.4322179555893}, {'caption': 'a man wearing a red shirt', 'bounding_box': [411.72650146484, 184.85931396484, 153.13218688965, 99.910217285156], 'confidence': -1.4512569904327}, {'caption': 'a man wearing a white shirt', 'bounding_box': [428.68530273438, 11.84511089325, 71.303901672363, 58.321235656738], 'confidence': -1.4559601545334}, {'caption': 'a woman with red hair', 'bounding_box': [575.03369140625, 9.784743309021, 109.36679840088, 43.838027954102], 'confidence': -1.488417506218}, {'caption': 'a yellow metal pole', 'bounding_box': [8.2413377761841, 281.42407226562, 351.45425415039, 35.010887145996], 'confidence': -1.4914002418518}, {'caption': 'a black and white bag', 'bounding_box': [32.384746551514, 315.16897583008, 740.62164306641, 3.0498881340027], 'confidence': -1.514389872551}, {'caption': 'a man holding a bat', 'bounding_box': [161.76858520508, 211.20889282227, 56.35591506958, 101.09882354736], 'confidence': -1.5749883651733}, {'caption': 'a person wearing a blue shirt', 'bounding_box': [465.50726318359, 226.53959655762, 138.03813171387, 81.73120880127], 'confidence': -1.6029547452927}, {'caption': 'a man wearing a blue shirt', 'bounding_box': [689.21600341797, 75.389671325684, 106.02407836914, 96.734184265137], 'confidence': -1.6299412250519}, {'caption': 'the hand of a woman', 'bounding_box': [601.67803955078, 225.80683898926, 61.300300598145, 85.30647277832], 'confidence': -1.676386475563}, {'caption': 'the man is wearing a red shirt', 'bounding_box': [712.38732910156, 227.85568237305, 84.22647857666, 53.029918670654], 'confidence': -1.8255370855331}, {'caption': 'the arm of a woman', 'bounding_box': [61.654350280762, 229.56631469727, 65.507713317871, 84.811706542969], 'confidence': -1.8722310066223}, {'caption': 'a yellow and black rope', 'bounding_box': [5.0727596282959, 269.62539672852, 110.31070709229, 45.607215881348], 'confidence': -1.9065618515015}, {'caption': 'a black metal pole', 'bounding_box': [300.94738769531, 285.47442626953, 96.13794708252, 29.865232467651], 'confidence': -1.9348530769348}, {'caption': 'a yellow and white shirt', 'bounding_box': [159.46556091309, 275.71237182617, 98.171806335449, 39.329788208008], 'confidence': -2.0727934837341}, {'caption': 'a red and white tennis shoe', 'bounding_box': [729.18243408203, 278.80197143555, 68.017372131348, 33.45991897583], 'confidence': -2.1597490310669}, {'caption': 'a black and white shirt', 'bounding_box': [2.4634728431702, 181.10147094727, 24.346759796143, 128.03912353516], 'confidence': -2.2653453350067}, {'caption': 'a black car', 'bounding_box': [300.97052001953, 249.05627441406, 52.036895751953, 60.876945495605], 'confidence': -2.2922360897064}, {'caption': 'a red and white jacket', 'bounding_box': [761.48016357422, 160.59843444824, 37.429588317871, 110.47760009766], 'confidence': -2.4671039581299}, {'caption': 'red and white bag', 'bounding_box': [767.70874023438, 223.01934814453, 31.639272689819, 84.200042724609], 'confidence': -2.8425395488739}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/densecap\",\n",
    "    files={\n",
    "        'image': open('party.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and label the apparent location of the scene within a given image or video. \n",
    "### e.g. outdoors in a garden, inside a kitchen, or around snowy mountains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': {'places': [{'confidence': 0.44, 'name': 'athletic field'}, {'confidence': 0.35, 'name': 'soccer field'}, {'confidence': 0.04, 'name': 'football field'}]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/places\",\n",
    "    files={\n",
    "        'image': open('garden.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waifu2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performs upscaling and denoising on an input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_url': 'https://api.deepai.org/job-view-file/99e87b92-5cce-49ae-a3c0-603bc3ed32d3/outputs/output.png'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/waifu2x\",\n",
    "    files={\n",
    "        'image': open('garden.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Talk 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizes the content of an image in a one sentence description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': 'a group of people standing in a room\\n'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/neuraltalk\",\n",
    "    files={\n",
    "        'image': open('party.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_url': 'https://api.deepai.org/job-view-file/02aaf5c2-4b14-45c1-b975-1a6c324c0966/outputs/output.jpg'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/CNNMRF\",\n",
    "    files={\n",
    "        'content_image': open('party.jpg', 'rb'),\n",
    "        'style_image': open('garden.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_url': 'https://api.deepai.org/job-view-file/2c03d4d7-fdc2-4cf9-acf0-d35d3618ce20/outputs/output.jpg'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/deepmask\",\n",
    "    files={\n",
    "        'content': open('kat.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'err': 'An error occurred processing this request. Please check your inputs and try again.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/neural-style\",\n",
    "    files={\n",
    "        'style': open('party.jpg', 'rb'),\n",
    "        'content': open('garden.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'err': 'An error occurred processing this request. Please check your inputs and try again.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/torch-srgan\",\n",
    "    files={\n",
    "        'image': open('party.jpg', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': '277448b0-21db-46b9-b6df-81b486baeeed'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
